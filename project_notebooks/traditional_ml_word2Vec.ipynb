{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tabulate import tabulate\n",
    "# %matplotlib inline\n",
    "# # import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "# from collections import Counter, defaultdict\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "\n",
    "# # Classifiers\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# # Evaluation\n",
    "# from sklearn import metrics\n",
    "\n",
    "GLOVE_6B_50D_PATH = \"C:\\\\Users\\\\Ukachi\\\\PycharmProjects\\\\paper_codes\\\\files\\\\glove_6B.txt\"\n",
    "GLOVE_27B_200D_PATH = \"C:\\\\Users\\\\Ukachi\\\\PycharmProjects\\\\paper_codes\\\\files\\\\glove.twitter.27B.200d.txt\"\n",
    "encoding=\"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    \"\"\"\n",
    "    This function is used to load a file from the specified file path\n",
    "    This was used to load the mapping dictionaries for this script\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: str\n",
    "\n",
    "    Returns\n",
    "    Any file\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        file = pickle.load(f)\n",
    "        return file\n",
    "    \n",
    "def store_data(filepath, data):\n",
    "    \"\"\"\n",
    "    This function is used for object serialization just to store what is going on\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: str The path where data is stored\n",
    "    data: The data being stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    pickle.dump(data, open(filepath, \"wb\"))\n",
    "    print(\"Data stored successfully\")\n",
    "    \n",
    "\n",
    "\n",
    "def split_data(data, label, percentage):\n",
    "    \"\"\"\n",
    "    This function is used to split the data\n",
    "    Args:\n",
    "        data: data\n",
    "        label: target\n",
    "        percentage: test size\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=percentage)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = load_file(\"C:\\\\Users\\\\Ukachi\\\\PycharmProjects\\\\paper_codes\\\\files\\\\imdb_data.pkl\")\n",
    "X = imdb_data.review\n",
    "y = imdb_data.sentiment\n",
    "\n",
    "\n",
    "nlp = English()\n",
    "token_list = []\n",
    "\n",
    "for review in X:\n",
    "    my_review = nlp(review)\n",
    "    for token in my_review:\n",
    "        token_list.append(token.text)\n",
    "\n",
    "all_words = set(token_list)\n",
    "print(len(all_words))\n",
    "    \n",
    "\n",
    "X_data = []\n",
    "\n",
    "for review in X:\n",
    "    my_doc = nlp(review)\n",
    "    token_review = []\n",
    "    for token in my_doc:\n",
    "        token_review.append(token.text)\n",
    "    X_data.append(token_review)\n",
    "    \n",
    "store_data(\"X_data.pkl\", X_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data(\"all_words.pkl\", all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(X_data)\n",
    "target = np.array(y)\n",
    "\n",
    "\n",
    "# Splitting the data set\n",
    "train_data, test_data, train_label, test_label = split_data(data, target, 0.2)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X_data, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "\n",
    "store_data(\"w2v.pkl\", w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the GLOVE files\n",
    "import struct \n",
    "\n",
    "glove_small = {}\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        nums = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_small[word] = nums\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_27B_200D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        nums=np.array(parts[1:], dtype=np.float32)\n",
    "        glove_big[word] = nums\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "1193514\n",
      "Data stored successfully\n",
      "Data stored successfully\n"
     ]
    }
   ],
   "source": [
    "print(len(glove_small))\n",
    "print(len(glove_big))\n",
    "\n",
    "store_data(\"glove_small.pkl\", glove_small)\n",
    "store_data(\"glove_big.pkl\", glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_glove_small = list(glove_small.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_glove_small[0: 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create the classifiers that use count vectorizers and TFIDF vectorizers\n",
    "# But since I have already done this I am going to move straight to the main thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X ,y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_etree_models = [\n",
    "    (\"etree_glove_small\", etree_glove_small),\n",
    "    (\"etree_glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "    (\"etree_glove_big\", etree_glove_big),\n",
    "    (\"etree_glove_big_tfidf\", etree_glove_big_tfidf),\n",
    "    (\"etree_w2v\", etree_w2v),\n",
    "    (\"etree_tfidf_w2v\", etree_w2v_tfidf)\n",
    "]\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_classifier(pipeline, classifier_name,\n",
    "                   X_train=train_data, y_train=train_label,\n",
    "                   X_test=test_data, y_test=test_label):\n",
    "    \"\"\"\n",
    "    This function is used to apply the classifiers and collate the results\n",
    "    Args:\n",
    "        clf: Machine Learning classifier\n",
    "        classifier_name: Description of classifier\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Working on {classifier_name}\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    print(metrics.classification_report(y_test, predictions, target_names=[\"Negative\", \"Positive\"]))\n",
    "    f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "    print(f\"F1-score for {classifier_name} = {f1_score}\")\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pipeline in all_etree_models:\n",
    "    results[name] = use_classifier(pipeline, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_2 = results\n",
    "\n",
    "print(etree_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results {\"IMDB_etree\":etree_2\n",
    "             \"IMDB_lr\":\n",
    "             \"IMDB_SVM\":\n",
    "             \"IMDB_NB\":\n",
    "             \"IMDB_DT\":\n",
    "             \"IMDB_RF\":\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data(\"etree_results\", etree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other algorithms used in the traditional_case\n",
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "lr_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"logistic regression\", LogisticRegression(max_iter=10000, tol=0.1, solver=\"lbfgs\"))])\n",
    "lr_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"logistic regression\", LogisticRegression(max_iter=10000, tol=0.1, solver=\"lbfgs\"))])\n",
    "lr_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"logistic regression\", LogisticRegression(max_iter=10000, tol=0.1, solver=\"lbfgs\"))])\n",
    "lr_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"logistic regression\", LogisticRegression(max_iter=10000, tol=0.1, solver=\"lbfgs\"))])\n",
    "lr_w2v = Pipeline([(\"w2v vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"logistic regression\", LogisticRegression(max_iter=10000, tol=0.1, solver=\"lbfgs\"))])\n",
    "lr_w2v_tfidf = Pipeline([(\"w2v vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"logistic regression\", LogisticRegression(max_iter=10000, tol=0.1, solver=\"lbfgs\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lr_models = [\n",
    "    (\"lr_glove_small\", lr_glove_small),\n",
    "    (\"lr_glove_small_tfidf\", lr_glove_small_tfidf),\n",
    "    (\"lr_glove_big\", lr_glove_big),\n",
    "    (\"lr_glove_big_tfidf\", lr_glove_big_tfidf),\n",
    "    (\"lr_w2v\", lr_w2v),\n",
    "    (\"lr_w2v_tfidf\", lr_w2v_tfidf)\n",
    "]\n",
    "\n",
    "lr_results = {}\n",
    "for name, pipeline in all_lr_models:\n",
    "    lr_results[name] = use_classifier(pipeline, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data(\"lr_w2v_results.pkl\", lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifiers\n",
    "# The other algorithms used in the traditional_case\n",
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    svm_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"SVM_{penalty}\", LinearSVC(penalty=penalty, tol=1e-3, dual=False))])\n",
    "    \n",
    "    svm_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"SVM_{penalty}\", LinearSVC(penalty=penalty, tol=1e-3, dual=False))])\n",
    "    \n",
    "    svm_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"SVM_{penalty}\", LinearSVC(penalty=penalty, tol=1e-3, dual=False))])\n",
    "    \n",
    "    svm_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"SVM_{penalty}\", LinearSVC(penalty=penalty, tol=1e-3, dual=False))])\n",
    "    \n",
    "    svm_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                            (f\"SVM_{penalty}\", LinearSVC(penalty=penalty, tol=1e-3, dual=False))])\n",
    "    \n",
    "    svm_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                            (f\"SVM_{penalty}\", LinearSVC(penalty=penalty, tol=1e-3, dual=False))])\n",
    "    \n",
    "    all_svm_models = [\n",
    "    (f\"svm_glove_small_{penalty}\", svm_glove_small),\n",
    "    (f\"svm_glove_small_tfidf_{penalty}\", svm_glove_small_tfidf),\n",
    "    (f\"svm_glove_big_{penalty}\", svm_glove_big),\n",
    "    (f\"svm_glove_big_tfidf_{penalty}\", svm_glove_big_tfidf),\n",
    "    (f\"svm_w2v_{penalty}\", svm_w2v),\n",
    "    (f\"svm_w2v_tfidf_{penalty}\", svm_w2v_tfidf)\n",
    "]\n",
    "\n",
    "    svm_results = {}\n",
    "    for name, pipeline in all_svm_models:\n",
    "        svm_results[name] = use_classifier(pipeline, name)\n",
    "    \n",
    "    store_data(f\"svm_results_{penalty}.pkl\", svm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_svm_models = [\n",
    "    (\"svm_glove_small\", svm_glove_small),\n",
    "    (\"svm_glove_small_tfidf\", svm_glove_small_tfidf),\n",
    "    (\"svm_glove_big\", svm_glove_big),\n",
    "    (\"svm_glove_big_tfidf\", svm_glove_big_tfidf),\n",
    "    (\"svm_w2v\", svm_w2v),\n",
    "    (\"svm_w2v_tfidf\", svm_w2v_tfidf)\n",
    "]\n",
    "\n",
    "svm_results = {}\n",
    "for name, pipeline in all_svm_models:\n",
    "    svm_results[name] = use_classifier(pipeline, name)\n",
    "    \n",
    "store_data(\"svm_results.pkl\", svm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sgd_glove_small_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.77      0.76      4939\n",
      "    Positive       0.77      0.75      0.76      5061\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n",
      "F1-score for sgd_glove_small_l2 = 0.7579835308294838\n",
      "Working on sgd_glove_small_tfidf_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.68      0.85      0.76      4939\n",
      "    Positive       0.81      0.60      0.69      5061\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     10000\n",
      "   macro avg       0.74      0.73      0.72     10000\n",
      "weighted avg       0.74      0.73      0.72     10000\n",
      "\n",
      "F1-score for sgd_glove_small_tfidf_l2 = 0.6903737259343148\n",
      "Working on sgd_glove_big_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.87      0.83      4939\n",
      "    Positive       0.86      0.78      0.82      5061\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "F1-score for sgd_glove_big_l2 = 0.8194949599916866\n",
      "Working on sgd_glove_big_tfidf_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.68      0.77      4939\n",
      "    Positive       0.75      0.91      0.82      5061\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     10000\n",
      "   macro avg       0.81      0.80      0.79     10000\n",
      "weighted avg       0.81      0.80      0.79     10000\n",
      "\n",
      "F1-score for sgd_glove_big_tfidf_l2 = 0.8194679241925439\n",
      "Working on sgd_w2v_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.85      0.85      4939\n",
      "    Positive       0.85      0.87      0.86      5061\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "F1-score for sgd_w2v_l2 = 0.8588223768002351\n",
      "Working on sgd_w2v_tfidf_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.85      0.86      4939\n",
      "    Positive       0.85      0.87      0.86      5061\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "F1-score for sgd_w2v_tfidf_l2 = 0.8614180929095355\n",
      "Data stored successfully\n",
      "Working on sgd_glove_small_l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.82      0.77      4939\n",
      "    Positive       0.80      0.69      0.74      5061\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n",
      "F1-score for sgd_glove_small_l1 = 0.7425491439441979\n",
      "Working on sgd_glove_small_tfidf_l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.84      0.75      4939\n",
      "    Positive       0.80      0.62      0.70      5061\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     10000\n",
      "   macro avg       0.74      0.73      0.73     10000\n",
      "weighted avg       0.74      0.73      0.73     10000\n",
      "\n",
      "F1-score for sgd_glove_small_tfidf_l1 = 0.7009096960284004\n",
      "Working on sgd_glove_big_l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.80      0.83      4939\n",
      "    Positive       0.82      0.86      0.84      5061\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "F1-score for sgd_glove_big_l1 = 0.8383118134515322\n",
      "Working on sgd_glove_big_tfidf_l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.57      0.70      4939\n",
      "    Positive       0.69      0.95      0.80      5061\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     10000\n",
      "   macro avg       0.80      0.76      0.75     10000\n",
      "weighted avg       0.80      0.76      0.75     10000\n",
      "\n",
      "F1-score for sgd_glove_big_tfidf_l1 = 0.8018040591330494\n",
      "Working on sgd_w2v_l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.81      0.84      4939\n",
      "    Positive       0.83      0.90      0.86      5061\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.85      0.85     10000\n",
      "\n",
      "F1-score for sgd_w2v_l1 = 0.8608218140503693\n",
      "Working on sgd_w2v_tfidf_l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.81      0.90      0.85      4939\n",
      "    Positive       0.89      0.80      0.84      5061\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "F1-score for sgd_w2v_tfidf_l1 = 0.8424122487240914\n",
      "Data stored successfully\n"
     ]
    }
   ],
   "source": [
    "# Create the classifiers\n",
    "# The other algorithms used in the traditional_case\n",
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    sgd_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"SVM_{penalty}\", SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))])\n",
    "    \n",
    "    sgd_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"SVM_{penalty}\", SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))])\n",
    "    \n",
    "    sgd_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"SVM_{penalty}\", SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))])\n",
    "    \n",
    "    sgd_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"SVM_{penalty}\", SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))])\n",
    "    \n",
    "    sgd_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                            (f\"SVM_{penalty}\", SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))])\n",
    "    \n",
    "    sgd_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                            (f\"SVM_{penalty}\", SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))])\n",
    "    all_sgd_models = [\n",
    "    (f\"sgd_glove_small_{penalty}\", sgd_glove_small),\n",
    "    (f\"sgd_glove_small_tfidf_{penalty}\", sgd_glove_small_tfidf),\n",
    "    (f\"sgd_glove_big_{penalty}\", sgd_glove_big),\n",
    "    (f\"sgd_glove_big_tfidf_{penalty}\", sgd_glove_big_tfidf),\n",
    "    (f\"sgd_w2v_{penalty}\", sgd_w2v),\n",
    "    (f\"sgd_w2v_tfidf_{penalty}\", sgd_w2v_tfidf)\n",
    "]\n",
    "\n",
    "\n",
    "    sgd_results = {}\n",
    "    for name, pipeline in all_sgd_models:\n",
    "        sgd_results[name] = use_classifier(pipeline, name)\n",
    "    \n",
    "    store_data(f\"sgd_results_{penalty}.pkl\", sgd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on bnb_glove_small\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.61      0.69      0.65      4939\n",
      "    Positive       0.66      0.58      0.61      5061\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     10000\n",
      "   macro avg       0.63      0.63      0.63     10000\n",
      "weighted avg       0.63      0.63      0.63     10000\n",
      "\n",
      "F1-score for bnb_glove_small = 0.6136387517074707\n",
      "Working on bnb_glove_small_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.69      0.67      4939\n",
      "    Positive       0.68      0.65      0.66      5061\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     10000\n",
      "   macro avg       0.67      0.67      0.67     10000\n",
      "weighted avg       0.67      0.67      0.67     10000\n",
      "\n",
      "F1-score for bnb_glove_small_tfidf = 0.6628768652928637\n",
      "Working on bnb_glove_big\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.75      0.73      4939\n",
      "    Positive       0.74      0.69      0.71      5061\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     10000\n",
      "   macro avg       0.72      0.72      0.72     10000\n",
      "weighted avg       0.72      0.72      0.72     10000\n",
      "\n",
      "F1-score for bnb_glove_big = 0.7130987793619858\n",
      "Working on bnb_glove_big_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.75      0.75      4939\n",
      "    Positive       0.75      0.75      0.75      5061\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "F1-score for bnb_glove_big_tfidf = 0.7503229011425734\n",
      "Working on bnb_w2v\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.79      0.72      4939\n",
      "    Positive       0.74      0.60      0.67      5061\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     10000\n",
      "   macro avg       0.70      0.70      0.69     10000\n",
      "weighted avg       0.70      0.69      0.69     10000\n",
      "\n",
      "F1-score for bnb_w2v = 0.6663755458515285\n",
      "Working on bnb_w2v_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.79      0.72      4939\n",
      "    Positive       0.74      0.60      0.67      5061\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     10000\n",
      "   macro avg       0.70      0.70      0.69     10000\n",
      "weighted avg       0.70      0.69      0.69     10000\n",
      "\n",
      "F1-score for bnb_w2v_tfidf = 0.6663755458515285\n",
      "Data stored successfully\n"
     ]
    }
   ],
   "source": [
    "bnb_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"mnb\", BernoulliNB(alpha=0.01))])\n",
    "    \n",
    "bnb_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"mnb\", BernoulliNB(alpha=0.01))])\n",
    "    \n",
    "bnb_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"mnb\", BernoulliNB(alpha=0.01))])\n",
    "    \n",
    "bnb_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"mnb\", BernoulliNB(alpha=0.01))])\n",
    "    \n",
    "bnb_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                            (f\"mnb\", BernoulliNB(alpha=0.01))])\n",
    "    \n",
    "bnb_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                            (f\"mnb\", BernoulliNB(alpha=0.01))])\n",
    "all_bnb_models = [\n",
    "    (f\"bnb_glove_small\", bnb_glove_small),\n",
    "    (f\"bnb_glove_small_tfidf\", bnb_glove_small_tfidf),\n",
    "    (f\"bnb_glove_big\", bnb_glove_big),\n",
    "    (f\"bnb_glove_big_tfidf\", bnb_glove_big_tfidf),\n",
    "    (f\"bnb_w2v\", bnb_w2v),\n",
    "    (f\"bnb_w2v_tfidf\", bnb_w2v_tfidf)\n",
    "]\n",
    "\n",
    "\n",
    "bnb_results = {}\n",
    "for name, pipeline in all_bnb_models:\n",
    "    bnb_results[name] = use_classifier(pipeline, name)\n",
    "    \n",
    "store_data(f\"mnb_results.pkl\", bnb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dt_glove_small\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.63      0.65      0.64      4939\n",
      "    Positive       0.65      0.64      0.64      5061\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     10000\n",
      "   macro avg       0.64      0.64      0.64     10000\n",
      "weighted avg       0.64      0.64      0.64     10000\n",
      "\n",
      "F1-score for dt_glove_small = 0.6421798582692884\n",
      "Working on dt_glove_small_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.63      0.64      0.63      4939\n",
      "    Positive       0.64      0.64      0.64      5061\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     10000\n",
      "   macro avg       0.64      0.64      0.64     10000\n",
      "weighted avg       0.64      0.64      0.64     10000\n",
      "\n",
      "F1-score for dt_glove_small_tfidf = 0.641434657683543\n",
      "Working on dt_glove_big\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.67      0.66      4939\n",
      "    Positive       0.67      0.65      0.66      5061\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     10000\n",
      "   macro avg       0.66      0.66      0.66     10000\n",
      "weighted avg       0.66      0.66      0.66     10000\n",
      "\n",
      "F1-score for dt_glove_big = 0.6601437699680512\n",
      "Working on dt_glove_big_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.65      0.65      4939\n",
      "    Positive       0.66      0.67      0.66      5061\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     10000\n",
      "   macro avg       0.66      0.66      0.66     10000\n",
      "weighted avg       0.66      0.66      0.66     10000\n",
      "\n",
      "F1-score for dt_glove_big_tfidf = 0.6642342964204714\n",
      "Working on dt_w2v\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.71      0.70      4939\n",
      "    Positive       0.71      0.69      0.70      5061\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     10000\n",
      "   macro avg       0.70      0.70      0.70     10000\n",
      "weighted avg       0.70      0.70      0.70     10000\n",
      "\n",
      "F1-score for dt_w2v = 0.7005582137161085\n",
      "Working on dt_w2v_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.71      0.70      4939\n",
      "    Positive       0.71      0.70      0.70      5061\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     10000\n",
      "   macro avg       0.70      0.70      0.70     10000\n",
      "weighted avg       0.70      0.70      0.70     10000\n",
      "\n",
      "F1-score for dt_w2v_tfidf = 0.7032200179443725\n",
      "Data stored successfully\n"
     ]
    }
   ],
   "source": [
    "dt_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"dt\", DecisionTreeClassifier())])\n",
    "    \n",
    "dt_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"dt\", DecisionTreeClassifier())])\n",
    "    \n",
    "dt_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"dt\", DecisionTreeClassifier())])\n",
    "    \n",
    "dt_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"dt\", DecisionTreeClassifier())])\n",
    "    \n",
    "dt_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                            (f\"dt\", DecisionTreeClassifier())])\n",
    "    \n",
    "dt_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                            (f\"dt\", DecisionTreeClassifier())])\n",
    "all_dt_models = [\n",
    "    (f\"dt_glove_small\", dt_glove_small),\n",
    "    (f\"dt_glove_small_tfidf\", dt_glove_small_tfidf),\n",
    "    (f\"dt_glove_big\", dt_glove_big),\n",
    "    (f\"dt_glove_big_tfidf\", dt_glove_big_tfidf),\n",
    "    (f\"dt_w2v\", dt_w2v),\n",
    "    (f\"dt_w2v_tfidf\", dt_w2v_tfidf)\n",
    "]\n",
    "\n",
    "\n",
    "dt_results = {}\n",
    "for name, pipeline in all_dt_models:\n",
    "    dt_results[name] = use_classifier(pipeline, name)\n",
    "    \n",
    "store_data(f\"dt_results.pkl\", dt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on rf_glove_small\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.75      0.74      4939\n",
      "    Positive       0.75      0.75      0.75      5061\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "F1-score for rf_glove_small = 0.7487606583382908\n",
      "Working on rf_glove_small_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.74      0.74      4939\n",
      "    Positive       0.74      0.73      0.74      5061\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     10000\n",
      "   macro avg       0.74      0.74      0.74     10000\n",
      "weighted avg       0.74      0.74      0.74     10000\n",
      "\n",
      "F1-score for rf_glove_small_tfidf = 0.7387978142076502\n",
      "Working on rf_glove_big\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.79      0.79      4939\n",
      "    Positive       0.80      0.79      0.80      5061\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     10000\n",
      "   macro avg       0.79      0.79      0.79     10000\n",
      "weighted avg       0.79      0.79      0.79     10000\n",
      "\n",
      "F1-score for rf_glove_big = 0.7951640075314637\n",
      "Working on rf_glove_big_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.79      0.79      4939\n",
      "    Positive       0.80      0.79      0.79      5061\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     10000\n",
      "   macro avg       0.79      0.79      0.79     10000\n",
      "weighted avg       0.79      0.79      0.79     10000\n",
      "\n",
      "F1-score for rf_glove_big_tfidf = 0.7943388756927949\n",
      "Working on rf_w2v\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.81      0.82      4939\n",
      "    Positive       0.82      0.83      0.82      5061\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "F1-score for rf_w2v = 0.8220472440944883\n",
      "Working on rf_w2v_tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.82      0.83      4939\n",
      "    Positive       0.83      0.84      0.84      5061\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "F1-score for rf_w2v_tfidf = 0.8350787902515415\n",
      "Data stored successfully\n"
     ]
    }
   ],
   "source": [
    "rf_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"rf\", RandomForestClassifier(n_estimators=200))])\n",
    "    \n",
    "rf_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                            (f\"rf\", RandomForestClassifier(n_estimators=200))])\n",
    "    \n",
    "rf_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"rf\", RandomForestClassifier(n_estimators=200))])\n",
    "    \n",
    "rf_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                            (f\"rf\", RandomForestClassifier(n_estimators=200))])\n",
    "    \n",
    "rf_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                            (f\"rf\", RandomForestClassifier(n_estimators=200))])\n",
    "    \n",
    "rf_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                            (f\"rf\", RandomForestClassifier(n_estimators=200))])\n",
    "all_rf_models = [\n",
    "    (f\"rf_glove_small\", rf_glove_small),\n",
    "    (f\"rf_glove_small_tfidf\", rf_glove_small_tfidf),\n",
    "    (f\"rf_glove_big\", rf_glove_big),\n",
    "    (f\"rf_glove_big_tfidf\", rf_glove_big_tfidf),\n",
    "    (f\"rf_w2v\", rf_w2v),\n",
    "    (f\"rf_w2v_tfidf\", rf_w2v_tfidf)\n",
    "]\n",
    "\n",
    "\n",
    "rf_results = {}\n",
    "for name, pipeline in all_rf_models:\n",
    "    rf_results[name] = use_classifier(pipeline, name)\n",
    "    \n",
    "store_data(f\"rf_results.pkl\", rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
